{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POMDP - Tiger Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tiger is put with equal probability behind one of two doors, while treasure is put behind the other one. You are standing in front of the two closed doors and need to decide which one to open. If you open the door with the tiger, you will get hurt (negative reward). But if you open the door with treasure, you receive a positive reward. Instead of opening a door right away, you also have the option to wait and listen for tiger noises. But listening is neither free nor entirely accurate. You might hear the tiger behind the left door while it is actually behind the right door and vice versa.\n",
    "\n",
    "![tiger_problem](imgs/tiger_problem.png)\n",
    "\n",
    "In today's lab, we will consider the following settings:\n",
    " - we start with an initial belief that the tiger is placed with equal probability behind each door. This means that our initial belief is a vector \\[0.5, 0.5\\]\n",
    " - the environment allows us to apply a finite number of actions\n",
    " - after applying left/right action, the tiger is placed again randomly behind one door, and the episodes continue till we reach the maximum number of steps allowed.\n",
    " \n",
    "Notice that the first two bullets will simplify the problem since the number of belief states that can be reached is finite. It is a well understood fact that most POMDP problems, even given arbitrary action and observation sequences of infinite length, are unlikely to reach most of the points in the belief simplex. Thus it seems unnecessary to plan equally for all beliefs, as exact algorithms do, and preferable to concentrate planning on most probable beliefs.\n",
    "\n",
    "This allows us to use the \"Point-based value iteration\" (PBVI) algorithm. PBVI approximates an exact value iteration solution by selecting a small set of representative belief points and then tracking the value and its derivative for those points only. The reader is referred to the paper [here](http://www.cs.cmu.edu/~ggordon/jpineau-ggordon-thrun.ijcai03.pdf).\n",
    "\n",
    "\n",
    "###  Implementation details\n",
    "\n",
    "#### Transition Probabilities\n",
    "\n",
    "| Prob. (LISTEN) | Tiger: left | Tiger: right |      | Prob. (LEFT)   | Tiger: left | Tiger: right |\n",
    "|----------------|-------------|--------------|      |----------------|-------------|--------------|\n",
    "| Tiger: left    |    1.0      |    0.0       |      | Tiger: left    |    0.5      |    0.5       |\n",
    "| Tiger: right   |    0.0      |    1.0       |      | Tiger: right   |    0.5      |    0.5       |\n",
    "\n",
    "\n",
    "| Prob. (Right)  | Tiger: left | Tiger: right |\n",
    "|----------------|-------------|--------------|\n",
    "| Tiger: left    |    0.5      |    0.5       |\n",
    "| Tiger: right   |    0.5      |    0.5       |\n",
    "\n",
    "We represented the transition probablility matrices as a 3D tensor, T, with the axis action(a), state(s), state(s').\n",
    "\n",
    "#### Observation Probabililites\n",
    "\n",
    "| Prob. (LISTEN) | O: TL | O: TR |      | Prob. (LEFT)   | O: TL | O: TR |\n",
    "|----------------|-------|-------|      |----------------|-------|-------|\n",
    "| Tiger: left    |  0.85 |  0.15 |      | Tiger: left    |  0.5  |  0.5  |\n",
    "| Tiger: right   |  0.15 |  0.85 |      | Tiger: right   |  0.5  |  0.5  |\n",
    "\n",
    "\n",
    "| Prob. (Right)  | O: TL | O: TR |\n",
    "|----------------|-------|-------|\n",
    "| Tiger: left    |  0.5  | 0.5   |\n",
    "| Tiger: right   |  0.5  | 0.5   |\n",
    "\n",
    "We represented the observation probablility matrices as a 3D tensor, O, with the axis action(a), state(s), observation(o).\n",
    "\n",
    "\n",
    "#### Immediate Rewards\n",
    "\n",
    "| Reward (LISTEN) |       |     | Reward (LEFT) |       |       | Reward (RIGHT) |       |\n",
    "|-----------------|-------|     |---------------|-------|       |----------------|-------|\n",
    "| Tiger: left     | -1    |     | Tiger: left   | -100  |       | Tiger: left    |  +10  |\n",
    "| Tiger: right    | -1    |     | Tiger: right  | +10   |       | Tiger: right   |  -100 |\n",
    "\n",
    "We represented the reward matrices as a 2D tensor, R, with the axis action(a), state(s).\n",
    "\n",
    "\n",
    "#### Observation\n",
    "\n",
    "Current implementation does not use vector operations. Fell free to replace for-loops by vector operation.\n",
    "Please referre to the [paper](http://www.cs.cmu.edu/~ggordon/jpineau-ggordon-thrun.ijcai03.pdf) for notations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we go ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "from itertools import count\n",
    "\n",
    "from env import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belief update\n",
    "\n",
    "![belief_update](imgs/belief_update.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_belief(b: np.ndarray, a: Actions, o: Obs, env: TigerEnv) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the next belief state from the current belief state, applied action\n",
    "    and received observation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        Current belief state\n",
    "    a\n",
    "        Applied action\n",
    "    o\n",
    "        Observation received\n",
    "    env\n",
    "        Tiger Environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Next belief state.\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract transition probability matrix\n",
    "    # adn observation probability matrix\n",
    "    T, O = env.T, env.O\n",
    "    \n",
    "    # get states, actions & observations\n",
    "    states, actions, obs = env.states, env.actions, env.obs\n",
    "    \n",
    "    # compute the next belief state\n",
    "    b_prime = np.zeros_like(b)\n",
    "    \n",
    "    ###########################\n",
    "    # TODO 1 - Your code here #\n",
    "    ###########################\n",
    "\n",
    "    for s_prime in states:\n",
    "        sum_over_states = 0\n",
    "        for s in states:\n",
    "            sum_over_states += T[a][s][s_prime] * b[s]\n",
    "        b_prime[s_prime] = O[a][s_prime][o] * sum_over_states\n",
    "\n",
    "    # normalize\n",
    "    b_prime /= b_prime.sum()\n",
    "    return b_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate belief states\n",
    "\n",
    "Most of the states will result in duplicates. It is important to remove the duplicates in the generation process. This can be implemented by computing the $L_p$ distance between two belief states. You are free to choose any norm you like (e.g. $L_1$, $L_2$, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(b: np.ndarray, buff: List[np.ndarray], eps: float = 1e-8) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether the belief is already in the buffer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        belief to check if it already exists\n",
    "    buff\n",
    "        buffer of beliefs to check against\n",
    "    eps\n",
    "        distance threshold\n",
    "    \"\"\"\n",
    "    dist = np.array([np.linalg.norm(b - x) for x in buff])\n",
    "    return any(dist < eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beliefs generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_beliefs(b: np.ndarray, env:TigerEnv) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates a list of possible beliefs that can result\n",
    "    form the current belief passed as argument\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        current belief\n",
    "    env\n",
    "        environment\n",
    "    \"\"\"\n",
    "    # get the list of possible actions\n",
    "    acts, obs = env.actions, env.obs\n",
    "    buff = []\n",
    "    \n",
    "    ######################\n",
    "    # TODO 2: Your code here\n",
    "    \n",
    "    # step 1: go through all the actions\n",
    "\n",
    "    for a in acts:\n",
    "        \n",
    "    # step 2: go through all the observations\n",
    "\n",
    "        for o in obs:\n",
    "    \n",
    "        # update current belief according to the\n",
    "        # current action and observation using \n",
    "        # the update_belief function previously implemented\n",
    "        # b_prime = ...\n",
    "\n",
    "            b_prime = update_belief(b, a, o, env)\n",
    "            \n",
    "            # add the new belief to the buffer only\n",
    "            # if it is not a duplicate\n",
    "            #   ...\n",
    "            if check_duplicate(b_prime, buff):\n",
    "                buff.append(b_prime)\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_beliefs(b_init: np.ndarray, env: TigerEnv) -> List[List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Generate all the possible belief that can result\n",
    "    in the maximum steps allowed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    b_init\n",
    "        initial belief (we're going to use the [0.5, 0.5] for this lab).\n",
    "    env\n",
    "        environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List of lists of belief, meaning that for each step\n",
    "    we will have a list of belief.\n",
    "    E.g.\n",
    "    [\n",
    "        [b_init],            ---> initial belief (level 1)\n",
    "        [b00, b01, b02, ...] ---> those result from the initial belief (level 2)\n",
    "        [b10, b11, b12, ...] ---> those result form the beliefs from the second level (level 3)\n",
    "        ....\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract the maximum number of steps allowe\n",
    "    # by the environment\n",
    "    max_num_steps = env.max_num_steps\n",
    "    \n",
    "    # initialize storing buffer by adding the \n",
    "    # list containing the initial belief\n",
    "    buff = [[b_init]]\n",
    "    \n",
    "    # for  the maximum steps allowed\n",
    "    for step in range(1, max_num_steps):\n",
    "        # buffer for the next level of beliefs\n",
    "        next_buff = []\n",
    "        \n",
    "        # go through all beliefs from the previous level\n",
    "        # and generate new ones\n",
    "        for b in buff[step - 1]:\n",
    "            # generate all the belief that can result for\n",
    "            # belief b (apply get_next_belief previously implemented)\n",
    "            tmp_buff = get_next_beliefs(b, env)\n",
    "            \n",
    "            # we have to check if the new beliefs\n",
    "            # don't exist already in the next level buffer\n",
    "            # so we don't add duplicates\n",
    "            for b_prime in tmp_buff:\n",
    "                if not check_duplicate(b_prime, next_buff):\n",
    "                    next_buff.append(b_prime)\n",
    "            \n",
    "        # add the new level of beliefs\n",
    "        buff.append(next_buff)\n",
    "    \n",
    "    return buff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (1)\n",
    "\n",
    "![pbvb1](imgs/pbvb1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_star(a: Actions, env: TigerEnv) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    a\n",
    "        current action\n",
    "    env\n",
    "        environment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    alpha^(a, *) vector. This in an array\n",
    "    of dimension: # of states and can be extracted\n",
    "    directly from the rewards matrix\n",
    "    \"\"\"\n",
    "    return env.R[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (2)\n",
    "\n",
    "![pbvb2](imgs/pbvb2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_o(a: Actions, o: Obs, V_prime: List[np.array], env: TigerEnv, gamma: float=0.9):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    a\n",
    "        action\n",
    "    o\n",
    "        observation\n",
    "    V_prime\n",
    "        list of alpha vectors from the next step\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discounting factor\n",
    "    \"\"\"\n",
    "    # get transition, observation and reward matrix\n",
    "    T, O, R = env.T, env.O, env.R\n",
    "    \n",
    "    # get posible states, actions and observations\n",
    "    states, actions, obs = env.states, env.actions, env.obs\n",
    "    \n",
    "    # buffer of next gamma_ao vectors\n",
    "    gamma_a_o = []\n",
    "    \n",
    "    # go through all alpha_prime vectors from V_prime\n",
    "    for alpha_prime in V_prime:\n",
    "        # define the new alpha_a_o vector\n",
    "        alpha_a_o = np.zeros((len(states)))\n",
    "        \n",
    "        ########################\n",
    "        # TODO 3: Your code here\n",
    "        # go throguh all states (s)\n",
    "\n",
    "        for s in states:\n",
    "        \n",
    "            # go through all states (s_prime)\n",
    "            for s_prime in states:\n",
    "        \n",
    "                # perform update alpha_a_o[s] += ...\n",
    "                alpha_a_o[s] += T[a][s][s_prime] * O[a][s_prime][o] * alpha_prime[s_prime]\n",
    "        \n",
    "        ########################\n",
    "        \n",
    "        # append the new alpha_a_o vector to the buffer\n",
    "        gamma_a_o.append(gamma * alpha_a_o)\n",
    "    \n",
    "    return gamma_a_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (3)\n",
    "\n",
    "![pbvb3](imgs/pbvb3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma_a_b(b: np.ndarray, \n",
    "                  V_prime: List[np.ndarray], \n",
    "                  env: TigerEnv, \n",
    "                  gamma: float=0.9) -> Dict[Actions, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b\n",
    "        belief state\n",
    "    V_prime\n",
    "        list of alpha vector from the next step\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discounting factor\n",
    "    \"\"\"\n",
    "    # get all posible actions and observations\n",
    "    A, O = env.actions, env.obs\n",
    "    \n",
    "    # define gamma_a_b buffer \n",
    "    gamma_a_b = {}\n",
    "    \n",
    "    # go through all actions\n",
    "    for a in A:\n",
    "        # get the gamma_a_star vectors form the previously implemented function\n",
    "        gamma_a_star = get_gamma_a_star(a, env)\n",
    "        \n",
    "        # define accumulator accumulator\n",
    "        sum_gamma_a_o = np.zeros_like(gamma_a_star, dtype=np.float)\n",
    "        \n",
    "        # go through all the observations\n",
    "        for o in O:\n",
    "            ########################################\n",
    "            # TODO 4: Your code here\n",
    "            # get gamma_a_o from the previously implementd function\n",
    "            # gamma_a_o = ...\n",
    "\n",
    "            gamma_a_o = get_gamma_a_o(a, o, V_prime, env, gamma)\n",
    "            \n",
    "            # need to do a maximization\n",
    "            best_alpha = None\n",
    "            best_score = -np.inf\n",
    "            \n",
    "            for alpha in gamma_a_o:\n",
    "            # go through all alphas from gamma_a_o\n",
    "                # compute the score by dot product between\n",
    "                # alpha and current belief b\n",
    "                # score = ...\n",
    "                score = np.dot(alpha, b)\n",
    "                # update the best score and alpha\n",
    "                # ...\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_alpha = alpha\n",
    "            \n",
    "            # add best alpha to the summation\n",
    "            # notice that if V_prime is empty (for the last step)\n",
    "            # we don't have any best_alpha\n",
    "            if best_alpha is not None:\n",
    "                sum_gamma_a_o += best_alpha\n",
    "            #########################################\n",
    "            \n",
    "        # add the reward vector to the accumulator\n",
    "        sum_gamma_a_o += gamma_a_star\n",
    "        \n",
    "        # store mapping between action and accumulator\n",
    "        gamma_a_b[a] = sum_gamma_a_o\n",
    "\n",
    "    return gamma_a_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (4)\n",
    "\n",
    "![pbvb4](imgs/pbvb4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V(B, V_prime, env: TigerEnv, gamma: float=0.9) -> Tuple[List[np.ndarray], Dict, List[float]]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    B\n",
    "        List of beliefs (per leve)\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discount factor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple containing the  the new V list, best policy for the current level, \n",
    "    and the best scores\n",
    "    \"\"\"\n",
    "    # define policy dictionary\n",
    "    policy = {}\n",
    "    \n",
    "    # define V and score buffers\n",
    "    V, scores = [], []\n",
    "    \n",
    "    # go through all beliefs\n",
    "    ################################################\n",
    "    # TODO 5: Your code here\n",
    "    for b in B:\n",
    "        # get gamma_a_b dictionary form the previous implemented function\n",
    "        # gamma_a_b = ....\n",
    "        gamma_a_b = get_gamma_a_b(b, V_prime, env, gamma)\n",
    "        \n",
    "        # variables for maximization\n",
    "        best_a = None\n",
    "        best_score = -np.inf\n",
    "        best_gamma_a_b = None\n",
    "        \n",
    "        # go through all actions from gamma_a_b\n",
    "        for a in gamma_a_b:\n",
    "            # compute score by dot product between\n",
    "            # the gamma_a_b corresponding to a and the current belief\n",
    "            # score = ...\n",
    "            score = np.dot(gamma_a_b[a], b)\n",
    "            \n",
    "            # update score if better and\n",
    "            # remeber the action and the alpha\n",
    "            #    ... update best score, best_a, best_gamma_a_b\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_a = a\n",
    "                best_gamma_a_b = gamma_a_b[a]\n",
    "            \n",
    "        \n",
    "        # remeber the action to be applied\n",
    "        # for the current belief state \n",
    "        policy[tuple(b)] = best_a\n",
    "        ###############################################\n",
    "        \n",
    "        # add best gamma_a_b to the V\n",
    "        V.append(best_gamma_a_b)\n",
    "        \n",
    "        # also remebere the best score\n",
    "        scores.append(best_score)   \n",
    "    \n",
    "    return V, policy, scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-based value backup (all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_based_value_backup(env: TigerEnv, \n",
    "                             gamma: float=0.9):\n",
    "    \"\"\"\n",
    "    Point-based value backup algorithm for POMDP\n",
    "    Link: http://www.cs.cmu.edu/~ggordon/jpineau-ggordon-thrun.ijcai03.pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env\n",
    "        environment\n",
    "    gamma\n",
    "        discount factor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Best policies per level. Each element\n",
    "    corresponds to a level\n",
    "    \"\"\"\n",
    "    \n",
    "    # define initial belief states\n",
    "    b_init = np.array([0.5, 0.5])\n",
    "    \n",
    "    # generate the list of all possible beliefs per level\n",
    "    B = generate_all_beliefs(b_init, env)\n",
    "    \n",
    "    # need to reverse the list cause we are starting\n",
    "    # from the last possible acton\n",
    "    B = reversed(B)\n",
    "    \n",
    "    # initail list of best gamma_a_b\n",
    "    V = []\n",
    "    \n",
    "    # buffer of policies and V vectors\n",
    "    policies = {}\n",
    "    \n",
    "    # for each level and each set of beliefs\n",
    "    for i, bs in enumerate(B):\n",
    "        # get the V's, policy and the best scores\n",
    "        V, policy, scores = get_V(bs, V, env)\n",
    "        \n",
    "        # store results\n",
    "        policies[env.max_num_steps - i - 1] = {\n",
    "            \"policy\": policy,\n",
    "            \"V\": V,\n",
    "            \"scores\": scores\n",
    "        }\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = TigerEnv(max_num_steps=3)\n",
    "\n",
    "# solve environment\n",
    "policies = point_based_value_backup(env=env, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the policy makes sense\n",
    "**TODO 6: interpret policy output for each value of the timestep horizon (env.max_num_steps)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step in range(env.max_num_steps):\n",
    "    print(\"========== Step %d ======== \" % (step, ))\n",
    "    pprint(policies[step]['policy'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize POMDP Value Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(policies, ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the agent in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "# you can change the number of steps\n",
    "env = TigerEnv(max_num_steps=4, noise=0.50)\n",
    "\n",
    "# solve environment\n",
    "policies = point_based_value_backup(env=env, gamma=0.9)\n",
    "print(policies[0])\n",
    "\n",
    "# do a few experiments\n",
    "scores = []\n",
    "num_experiments = 10\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    \n",
    "    # initial belief state\n",
    "    b = np.array([0.5, 0.5])\n",
    "    \n",
    "    # score variable & buffer actions\n",
    "    score = 0\n",
    "    acts, obs = [], []\n",
    "    \n",
    "    for step in count():\n",
    "        # interact with the environment\n",
    "        b = get_closest_belief(policies[step][\"policy\"], b)\n",
    "        a = policies[step][\"policy\"][tuple(b)]\n",
    "        o, r, done, _ = env.step(a)\n",
    "        \n",
    "        # update score, acts & obs\n",
    "        score += r\n",
    "        acts.append(a)\n",
    "        obs.append(o)\n",
    "        \n",
    "        # break if environment completed\n",
    "        if done:\n",
    "            acts = [env.action_mapping[a] for a in acts]\n",
    "            obs = [env.obs_mapping[o] for o in obs]\n",
    "            \n",
    "            print(\"Episode %d, Score: %.2f\" % (i, score))\n",
    "            print(\"\\t* Actions:\", acts)\n",
    "            print(\"\\t* Obs:\", obs)\n",
    "            print(\"\\n\")\n",
    "            break\n",
    "        \n",
    "        # update belief\n",
    "        b = update_belief(b=b, a=a, o=o, env=env)\n",
    "    \n",
    "    # save score\n",
    "    scores.append(score)\n",
    "    \n",
    "# report mean score\n",
    "print(\"=================\")\n",
    "print(\"Avg score: %.2f\" % (np.mean(scores), ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
